# 第10章 基于增强学习的视频游戏

与输入输出一一对应的监督学习不同，增强学习是另一种最大化问题：在给定环境下，寻找出一个行为策略以达到回报最大化（行为会互相作用，甚至会改变环境）。增强学习算法的目标不是一个明确而严格的结果（译者注：如分类回归问题的具体值，是一个明确而固定的数值），而是最大化最终得到的回报。此类算法会通过自由的反复试错来达成目标。如同幼童学步一般，算法会在实验环境中分析行为带来的反馈，然后找出实现最优回报的方式。这也类似人类尝试新游戏的情形：尝试寻求最优策略，在此之后尝试许多方法，然后最终决定行为准则。

迄今为止，没有增强学习算法能够在通用学习上能够媲美人类。较之算法，人类可以从更少的输入中学习，并且能够在非常复杂多样，结构有无的许多环境中进行学习。但是增强学习在一些特定问题中表现出了超出人类的能力（是的，他们的表现优于人类）。在特定游戏中，假设训练时间充足，增强学习算法能够给出令人惊叹的结果（比如AlphaGo https://deepmind.com/research/alphago/ ——第一个在围棋这种需要长期战略与直觉的复杂游戏中打败了世界冠军的程序）。

在本章中，我们将会呈现给您一个富有挑战性的项目：使用增强学习在雅达利游戏机上的“登月飞行器”游戏中学习如何正确使用指令。鉴于此游戏具有较少的指令，并可以根据少数几个数值描述游戏场景（甚至不用去看屏幕图像就能理解需要做什么。事实上此游戏第一版诞生于20世纪60年代，而且是纯文本的）并完成游戏，并且现在增强学习算法能够成功的解决它。

神经网络和增强学习刚刚开始交融。在20世纪90年代早期，IBM公司的格里·特索罗（Gerry Tesauro，应为Gerald Tesauro）研究员编写了著名的TD-Gammon，结合了前馈网络与时间差分学习（一种蒙特卡洛与动态规划的结合算法）来训练TD-Gammon进行西洋双陆棋游戏。西洋双陆棋是一个双人使用若干个骰子进行的游戏，如果读者希望进一步了解这个游戏，可以通过以下美国双陆棋联盟的网站来了解它的规则：http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rulesof-backgammon/ 当时TD-Gammon在西洋双陆棋上有着较好的表现是受益于西洋双陆棋是一个基于掷骰子的非确定性游戏，但当时无法在更具有确定性的游戏中获得较好的结果。近些年来，得益于谷歌深度学习的研究者，神经网络被证明能够帮助解决西洋双陆棋以外的问题，并且它在任何人的计算机上运行。最近，强化学习被列于未来深度学习乃至机器学习的爆点榜单之首。读者可以从下述链接访问谷歌大脑的人工智能科学家Ian Goodfellow的榜单，并看到强化学习位于榜首：https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002



## 关于游戏

“登月飞行器”是一个1979年左右在雅达利游戏机上的街机游戏。游戏发布载体为特殊设计的匣子，游戏画面基于黑白矢量图形，为一个月球着陆舱接近月球表面的侧视图。着陆舱需要在若干个指定的地点之一着陆。由于地形差异，这些着陆点具有不同的宽度和难度，在不同的着陆点着陆也会具有不同的得分。通过界面玩家能够得知飞船的高度、速度、燃料余量、得分以及已用时间。在月球引力作用下，玩家需要通过消耗燃料调节着陆舱的旋转与反推（需要考虑惯性）来使得着陆舱着陆。燃料是这个游戏的关键。

当着陆舱燃料耗尽并接触到月面时游戏结束。在燃料耗尽之前，就算着陆舱已经坠毁，玩家也能继续游戏。玩家可用的指令有四个：左转、右转、推进，以及放弃着陆。当玩家使用推进命令时，着陆器将使用底部的推进器将着陆舱向其前方加速。而使用放弃着陆命令时，飞船将会调整姿态为头部朝上，并进行一次强力的推进以避免坠毁。

这个游戏的有趣之处在于代价与回报非常清楚，但有些是显而易见的（比如尝试着陆中消耗的油料），有些则被推迟到了着陆舱着陆的时候（只有当着陆完全停止时，你才知道着陆成功与否）。操作着陆舱着陆需要经济的规划燃料的使用，尽量不要过于浪费。着陆会提供一个得分，当着陆越困难、越安全的时候，这个得分会越高。

--human_flag--

## OpenAI版

正如在其网站上提供的文件(https：/gyc.openai.com/)中所述，  OpenAI GUM是一个开发和比较强化学习算法的工具包。  该工具包实际上包括一个运行Python 2和Python 3的Python包，以及网站API，后者用于上传您自己的算法的性能结果，并将它们与其他算法进行比较(实际上，我们将不探讨工具包的一个方面)。  这个工具包体现了强化学习的原则，在这里您有一个环境和一个代理：代理可以在环境中执行操作或不执行操作，而环境将以新的状态(表示环境中的情况)和奖励来进行响应，奖励是一个分数，告诉代理它是否做得好。GEM工具包为所有东西提供了环境，因此，您必须使用一种算法来编码代理，以帮助代理面对环境。环境由env处理，env是一个带有强化学习方法的类，当您发出命令为特定游戏创建它时，它会被实例化： gym.make('environment').让我们看一下官方文档中的一个示例： 

```python
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20):
	observation = env.reset()
	for t in range(100):
		env.render()
		print(observation)
		# taking a random action
		action = env.action_space.sample()
		observation, reward, done, info = \
							env.step(action)
		If done:
			print("Episode finished after %i \
					timesteps" % (t+1))
			break
```

在本例中，运行环境是CartPol-V0。主要是一个控制问题，在CartPol-V0游戏中，一个摆在一辆沿着摩擦较少的轨道上运动的小车上。游戏的目的是通过将向前或向后的力量施加到手推车上来保持钟摆尽可能长的直立，你可以通过在youtube上观看这个序列来观察游戏的动态，这是在IIT Madras的动力学和控制实验室进行的一个现实生活实验的一部分，并且基于能解决困难控制问题的类似神经元的自适应元素：https:/www.youtu.com/Watch?v=qMlc43-sc-lg。 

卡尔顿问题在神经元中被描述为能解决困难的学习控制问题的自适应元素(http：/ieeheur.org/Document/6313077/)，由Barto，AndrewG.；Sutton，RichardS.；Anderson，Charles W.在ieee关于系统、人和控制论的事务中描述。  下面是在示例中应用的env方法的简要说明：  reset()：这会将环境的状态重置为初始默认条件。它实际上返回了START的观察结果。  步骤(操作)：这会将环境移动一个时间步骤。它返回一个由变量组成的四值向量：观察、奖励、完成和信息。观察是环境状态的一种表示，在每一场游戏中都用不同的价值向量来表示。例如，在一个涉及物理的游戏中，例如CartPol-V0，返回的向量由购物车的位置、购物车的速度、杆的角度和杆的速度组成。奖励只是前一个动作所取得的分数(为了计算出每个点的总分数，您需要对奖励进行汇总)。所做的变量是一个布尔标志，它告诉您在游戏中是否处于终端状态(游戏结束)。INFO将提供诊断信息，这些信息不是用于您的算法，而是用于调试。  呈现(MODE=‘人类’，CLOSE=FALSE)：这将呈现环境的一个时间框架。默认模式会做一些对人类友好的事情，比如弹出一个窗口.。传递关闭标志指示呈现引擎关闭任何此类窗口。  这些命令产生的效果如下：  设置CartPol-V0环境。  运行1，000个步骤。  随机选择对购物车施加正向或负向力，将结果可视化。  这种方法的有趣之处在于，您可以轻松地更改游戏，只需将一个不同的字符串提供给绞尽脑汁的.make方法(例如，尝试MsPacman-V0或突破-V0，或者从清单中选择任何您可以获得的内容，这些列表都可以通过绞尽脑汁获得)，并且在不改变代码的情况下测试解决不同环境的方法。OpenAI GUM通过在所有环境中使用公共接口，可以很容易地测试您的算法对不同问题的泛化能力。此外，它还为您根据该模式推理、理解和解决代理环境问题提供了一个框架。在t-1时，一个状态和奖励被推送给一个代理，该代理与一个动作发生反应，产生一个新的状态，并在t时产生一个新的奖励：  图1：环境和代理如何通过状态、行为和奖励进行交互。  在OpenAI健身房中的每一个不同的游戏中，动作空间(代理响应的命令)和观察空间(表示状态)都会发生变化。您可以通过使用一些打印命令来查看它们是如何更改的，就在您设置了一个环境之后：  print(env.action_space)print(env.observation_space)print(env.view_space.High)打印(env.observation_space.low)。  在Linux上安装OpenAI(Ubuntu14.04或16.04)。  我们建议在Ubuntu系统上安装环境。OpenGym AI是为Linux系统创建的，几乎没有对Windows的支持。根据您的系统以前的设置，您可能需要先安装一些附加的东西：  APT-get install-y python 3-dev pylibjpeg-dev libjpeg-dev-dev使golang tmux htopCr-浏览器git cmakezlib1gdev libjpeg-dev xvfb libav-toolxorg-dev python-opengl libost-all-dev libsdl2-dev-swg。  我们建议与Anaconda合作，所以也安装Anaconda 3。您可以在https：/www.anaconda.com/下载/上找到有关安装这个Python发行版的所有信息。  在设置系统需求之后，安装OpenGym AI及其所有模块非常简单： 

对于这个项目，我们实际上对Box2D模块感兴趣，它是一个2D物理引擎，在2D环境中提供真实世界物理的渲染，就像虚拟现实视频游戏中常见的那样。您可以通过在Python中运行以下命令来测试Box2D模块是否工作：  进口健身房。  env=unit.make(‘renarlander-v2’)。  reet()env.赋值()。  如果所提供的代码运行时没有问题，则可以继续该项目。在某些情况下，Box2D可能会变得难以运行，例如，可能会出现HTTPS：/GitHub.com/cbfinn/gps/Issues/3-4中报告的问题，尽管还有许多其他示例。我们发现，在基于Python3.4的Conda环境中安装Gym使事情变得简单得多：  Conda Create-名为gyc python=3.4 anaconda gcc=4.8.5。  源激活健身房。  Conda安装PIP 6 libgcc swg Conda安装-c Conda-锻OpenCV pip安装-升级TensorFlow-GPU git克隆https：/github.com/OpenAI/健身房。  CD健身房pip安装-e.conda安装-c https：/conda.anaconda.org/kne pybox 2d。  这个安装序列应该允许您创建一个Conda环境，该环境适合我们将在本章中介绍的项目。  月球登陆器在OpenAI健身房。  umarlander-v2是由OpenAI的工程师奥列格·克里莫夫(OlegKlimov)开发的场景，灵感来自最初的Atari月球着陆器(https：/GitHub.com/olegklimo)。在实现中，您必须将您的着陆舱带到始终位于坐标x=0和y=0的月台。此外，您的实际x和y位置是已知的，因为它们的值存储在状态向量的前两个元素中，状态向量包含用于强化学习算法的所有信息，以决定在某个时刻采取的最佳行动。  这使任务变得容易理解，因为您不必处理与目标(机器人技术中的一个常见问题)相关的位置的模糊或不确定的定位。  在每个时刻，着陆舱有四种可能的行动可供选择：  一事无成。  左转。  旋转右推力。  然后有一个复杂的奖励系统使事情变得有趣：  从屏幕顶部移动到着陆平台，达到零速度范围从100到140点的奖励(在着陆平台外着陆是可能的)。  如果着陆舱在没有停止的情况下离开着陆台，它将失去先前的一些奖励。  每集(用来指游戏会话的术语)在着陆舱崩溃或停止时完成，分别提供额外的-100或。  +100点。  与地面接触的每条腿为+10。 

点燃主引擎是-0.3点每帧(但燃料是无限的)解决插曲给予200点。  这个游戏与离散命令(它们实际上是二进制的：全推力或无推力)配合得很好，因为正如模拟的作者所说，根据庞蒂亚金的最大原理，最好是全速启动或完全关闭引擎。  该游戏也是可解的，使用一些简单的启发式基于距离的目标，并使用比例积分导数(PID)控制器来管理下降的速度和角度。PID是一种工程解决方案的控制系统，你有反馈。在以下URL，您可以获得关于它们如何工作的更详细说明：https：/www.csimn.com/csi_pages/PIDforDummies.。  通过深度学习探索强化学习。  在这个项目中，我们对开发一种启发式(一种仍然有效的方法来解决人工智能中的许多问题)或构造一个工作的PID不感兴趣。相反，我们打算利用深度学习为特工提供必要的情报，以成功地操作月球登陆者视频游戏会话。  强化学习理论为解决这类问题提供了几个框架：  基于价值的学习：这是通过找出处于某种状态的回报或结果而起作用的。通过比较不同可能状态下的奖赏，选择导致最佳状态的行为。Q学习就是这种方法的一个例子。  基于策略的学习：根据来自环境的奖励来评估不同的控制策略。它取决于达到最佳效果的政策。  结果。  基于模型的学习：这里的想法是在代理内复制环境模型，从而允许代理模拟不同的行为及其相应的奖励。  在我们的项目中，我们将使用基于价值的学习框架；具体来说，我们将使用现在的经典方法来加强基于q-学习的学习，这种方法已经成功地控制了游戏，在游戏中，代理必须决定一系列将导致游戏后期延迟奖励的动作。该方法由C.J.C.H.Watkins于1989年在他的博士论文中设计，也称为Q-学习，是基于这样一种理念，即一个Agent在考虑到当前状态的情况下在环境中工作，以定义一系列将导致最终回报的行为： 

 在上面的公式中，它描述了一个状态在一个动作a之后是如何导致一个奖励r和一个新的状态s的。从游戏的初始状态开始，该公式应用一系列动作，这些动作一个接一个地转换每个后续状态，直到游戏结束。然后，您可以将游戏想象为一系列动作所链接的状态。然后，您还可以解释上述公式如何通过一系列动作a将初始状态s转换为最终状态s‘和最终奖励r。  在强化方面，策略是如何最好地选择我们的行动顺序。策略可以用一个称为q的函数逼近，因此给定当前状态s和可能的动作a，作为输入，它将提供从该动作得到的最大报酬r的估计：  这种方法显然是贪婪的，这意味着我们只是在精确的状态下选择最佳的操作，因为我们期望总是在每一步中选择最佳的操作将导致最好的结果。因此，在贪婪的方法中，我们不考虑可能导致奖励的行为链，而只考虑下一个行为，a。不过，可以很容易地证明，如果符合以下条件，我们可以有信心地采取贪婪的做法，并利用这种政策获得最大的回报：  我们找到了一个完美的策略先知，Q\*我们在一个信息完美的环境中工作(这意味着我们可以了解关于环境的一切)环境遵循马尔可夫原理(见提示框)。  马尔可夫原理指出，未来(状态，回报)只取决于现在，而不是过去，因此，我们可以简单地通过观察现状而忽略以前发生的事情来获得最好的结果。  事实上，如果我们将Q函数构建为一个递归函数，我们只需要探索(使用广度优先搜索方法)对要测试的操作的当前状态的影响，并且递归函数将返回最大可能的回报。  这种方法在计算机模拟中非常有效，但在现实世界中却毫无意义：  环境大多是概率的。即使你采取了行动，你也不一定能得到确切的回报。  环境与过去联系在一起，单靠现在无法描述未来，因为过去可能会产生隐藏的或长期的后果。  环境不是完全可以预测的，所以你不能事先知道一个行动的回报，但是你可以在事后知道它们(这被称为后验条件)。  环境非常复杂。你不能在一个合理的时间内计算出一个行动的所有可能的后果，因此你不能确定一个行动产生的最大回报。  然后，解决方案是采用一个近似Q函数，它可以考虑概率结果，并且不需要通过预测来探索所有的未来状态。显然，它应该是一个真正的近似函数，因为在复杂的环境中构建值搜索表是不切实际的(有些状态空间可能需要连续的值，从而使可能的组合变得无限)。此外，该函数可以离线学习，这意味着利用代理的经验(那时记忆能力变得非常重要)。  以前也有人尝试过用神经网络来逼近q函数，但唯一成功的应用是td_gammon，这是一个仅由多层感知器驱动的强化学习的双子棋程序。TD_Gammon达到了超人水平的游戏，但在当时，它的成功不能复制在不同的游戏，如国际象棋或围棋。  这导致了一种信念，即神经网络并不真正适合于计算出一个Q函数，除非游戏是某种随机的(你必须在双子棋中掷骰子)。2013年，一篇关于深度强化学习的论文，以深度强化学习（Atari）为题，Volodymyr Minh等人应用于旧Atari游戏证明相反。  这篇论文演示了如何使用神经网络学习Q函数来玩一系列的Atari街机游戏(例如，梁骑士、突破、恩杜罗、乒乓球、Q*伯特、Seaquest和空间入侵者)，只需处理视频输入(在60 Hz的210×160 RGB视频中采样帧)并输出操纵杆和射击按钮命令。本文将这种方法命名为深Q网络(DQN)，并介绍了经验回放和勘探与开发的概念，我们将在下一节中讨论这些概念。当尝试将深度学习应用于强化学习时，这些概念有助于克服一些关键问题： 

 没有足够的例子可供学习-这是强化学习所必需的，在使用深度学习时更是不可或缺的。  在行动和有效奖励之间延长的延迟，这需要在获得奖励之前处理一系列可变长度的进一步行动。  一系列高度相关的动作序列(因为动作通常会影响到下面的动作)，这可能导致任何随机梯度下降算法过度适应最新的示例，或者只是非最优收敛(随机梯度下降期望随机示例，而不是相关示例)。  Mnih和其他研究人员的论文“通过深度强化学习实现人类层面的控制”(http：/www.davidqiu.com：8888/Research/Nature14236.pd f)只是证实了DQN的有效性，即利用DQN开发更多的游戏，并将DQN的性能与人类玩家和经典强化学习算法进行比较。  在许多游戏中，dqn被证明比人类的技能更好，尽管长期策略仍然是算法的一个问题。在某些游戏中，比如“突破”，经纪人发现了一些狡猾的策略，比如挖一条隧道穿过墙壁，以轻松的方式把球送进并摧毁墙壁。在其他的比赛中，比如蒙特祖马的复仇，这位经纪人仍然一无所知。  在本文中，作者详细地讨论了Agent如何理解赢得突围游戏的细节，并给出了DQN函数的响应图，展示了如何将高回报分数分配给先在墙上挖一个洞然后让球通过的行为。 

 深入学习Q的技巧和技巧。  神经网络获得的Q-学习被认为是不稳定的，直到一些技巧使之成为可能和可行。在深度Q学习中有两个马力马，尽管最近开发了其他的算法变体，以解决原解的性能和收敛性问题。这些新的变体在我们的项目中没有被讨论：双重Q-学习，延迟Q-学习，贪婪的GQ，和快速的Q-学习.。我们将要探索的两个主要动力马是经验重放和勘探与开发之间不断减少的权衡。  通过经验重播，我们简单地将所观察到的游戏状态存储在一个预先确定大小的队列中，因为当队列满时，我们将丢弃旧的序列。在存储的数据中，我们期望有一些元组，包括当前状态、应用操作、结果得到的状态和获得的奖励。如果我们考虑一个由当前状态和动作组成的简单元组，我们就可以观察到在环境中运行的代理，我们可以考虑结果状态和奖励的根本原因。现在，我们可以考虑元组(当前状态和动作)作为关于奖励的预测器(x向量)。因此，我们可以使用与行动直接相关的奖励，以及在游戏结束时将获得的奖励。  给定这样的存储数据(我们可以算出作为代理的内存)，我们对其中的一些数据进行采样，以便创建一个批处理，并使用获得的批处理来训练我们的神经网络。但是，在将数据传递给网络之前，我们需要定义我们的目标变量y向量。由于抽样状态大多不是最终状态，所以我们可能会有一个零奖励或只是部分奖励来匹配已知的输入(当前状态和选择的动作)。部分奖励不是很有用，因为它只是讲述了我们需要知道的故事的一部分。我们的目标，事实上，是知道在游戏结束时，我们将得到的全部奖励，在采取行动后，我们目前的状态，我们正在评估(我们的x值)。  在这种情况下，由于我们没有这样的信息，我们只是尝试使用我们现有的Q函数来近似这个值，以便估计剩余报酬，这将是我们正在考虑的(状态，动作)元组的最大结果。在得到它之后，我们用Bellman方程对它的值进行折现。 

在谷歌的软件工程师萨尔·坎迪多博士的这篇优秀教程中，你可以读到关于这种现在经典的强化学习方法的解释：http：/Robotics.AI.UIUC.Edu/~scandido/？Developing_Reinforcement_Learning_from_the_Bellman_Equatio n)，其中将当前的奖励添加到未来的折扣奖励中。  使用小值(接近于零)的折扣使Q函数更倾向于短期奖励，而使用高折扣值(接近1)使Q函数更面向未来收益。  第二个非常有效的技巧是利用系数来进行勘探和开发之间的交易。在探索中，期望代理人尝试不同的行动，以找到给定特定状态下的最佳行动方案。在利用过程中，代理利用它在以前的探索中学到的知识，并简单地决定在这种情况下应该采取什么最好的行动。  在勘探和开发之间找到一个好的平衡，与我们前面讨论的经验回放的使用密切相关。在开始对DQN算法进行优化时，我们只需依赖于一组随机的网络参数。这就像我们在本章的简单介绍性示例中所做的那样，抽样随机操作。在这种情况下，代理将探索不同的状态和操作，并帮助形成初始Q函数。对于复杂的游戏，例如月球着陆器，使用随机选择不会使代理走得太远，而且从长远来看，它甚至可能会变得没有效率，因为它将阻止代理学习状态元组和操作的预期奖励，只有在代理之前做了正确的事情时才能访问这些状态和操作。事实上，在这种情况下，DQN算法将很难找到适当地分配正确的奖励给一个行动，因为它将永远不会看到一个完整的游戏。由于游戏是复杂的，它不太可能被解决的随机序列的行动。  正确的方法，那么，是平衡学习的机会和使用已学会的东西，以进一步的代理人在游戏中的问题仍有待解决。这类似于通过一系列连续的近似找到一个解决方案，每次让代理更接近于安全和成功着陆的正确动作序列。因此，代理人应该先偶然地学习，找出在特定情况下要做的最好的事情，然后应用所学到的东西，进入新的情况，通过随机选择，这些新情况也将被依次解决、学习和应用。  这是使用递减值作为阈值，让代理决定在游戏中的某个点上，是否采取随机选择，看看发生了什么，或者利用到目前为止学到的知识，并利用它在这一点上做出最佳的操作，这是它的实际能力。从均匀分布[0，1]中选取一个随机数，代理将其与epsilon值进行比较，如果随机数大于epsilon，则使用其近似神经Q函数。否则，它将从可用的选项中选择一个随机操作。在此之后，它将减少epsilon的数目。最初，epsilon被设置为最大值1.0，但取决于衰减因子，它将随着时间的推移而或多或少地迅速减小，得到一个不应该是零(不可能随机移动)的最小值，以便总是有可能通过意外的意外(最小的开放因素)学习新的和意想不到的东西。 

深入理解Q-学习的局限性。  即使在深入学习q的情况下，也存在一些限制，无论您是通过视觉图像或其他关于q函数的观察来近似q函数。  环境：  这种近似需要很长的时间才能收敛，有时它并不能很顺利地实现：你甚至可能看到神经网络的学习指标在很多时期都在恶化，而不是变得更好。  基于贪婪的方法，Q-学习提供的方法与启发式方法没有什么不同：它指出了最好的方向，但不能提供详细的规划。当处理长期目标或必须明确表达为次级目标的目标时，Q-学习表现得很糟糕。  Q学习如何运作的另一个结果是，它并不是从一般的角度来理解游戏的动态，而是从一个特定的角度来理解游戏的动态(它复制了它在训练中所体验到的有效的东西)。因此，任何引入游戏的新颖性(在训练过程中从未真正体验过)都会破坏算法，使其完全无效。在算法中引入新游戏时也是如此，它根本无法执行。  启动项目。  在经过长时间的强化学习和DQN方法之后，我们终于准备好开始编码了，对如何操作OpenAI GEM环境和如何设置Q函数的DQN近似有了所有的基本理解。我们只是开始导入所有必要的包：  进口健身房。  从健身房进口包装进口numpy作为NP进口随机，临时文件，os从集合进口deque进口天索夫作为TF。  temfile模块生成临时文件和目录，这些文件和目录可以用作数据文件的临时存储区域。deque命令从集合模块创建一个双头队列，实际上是一个列表，您可以在列表的开头或结尾追加项。有趣的是，它可以设置为预定义大小。当已满时，旧项将被丢弃，以便为新条目创建位置。  我们将使用一系列表示代理、代理的大脑(我们的DQN)、代理的内存和环境的类来构造这个项目，这些类由OpenAI GUM提供，但它需要正确地连接到代理。有必要为此编写一个类。 

 定义人工智能的大脑。  该项目的第一步是创建一个包含所有神经网络代码的Brain类，以计算Q值近似。这个类将包含必要的初始化、用于创建合适的TensorFlow图的代码、一个简单的神经网络(不是一个复杂的深度学习体系结构，而是一个用于我们项目的简单的、工作的网络-您可以用更复杂的体系结构替换它)，以及最后一个适合和预测操作的方法。  我们从初始化开始。作为输入，首先，我们真的需要知道与我们从游戏中得到的信息相对应的状态输入(NS)的大小，以及与我们可以在游戏中执行操作的按钮相对应的动作输出(NA)的大小。可以选择，但强烈建议，我们也必须设置范围。为了定义范围，字符串将帮助我们保持为不同目的创建的单独网络，在我们的项目中，我们有两个，一个用于处理下一个奖励，一个用于猜测最终奖励。  然后，我们必须定义优化器的学习速率，这是一个亚当。  Adam优化器在以下文章中进行了描述：https：/ / arxiv。 org / abs / 1412。 6980.它是一种非常有效的基于梯度的优化方法，只需很少的调整即可正常工作。ADAM优化是一种随机梯度下降算法，类似于。  有动力的。这篇文章，https：/theberkeleyview。  com/2015/11/19/berkeleyview-for-adam-a-Method-for随机优化/，载于加州大学伯克利分校计算机视觉评论快报，提供更多信息。根据我们的经验，分批训练深度学习算法是最有效的解决方案之一，需要对学习速率进行调整。  最后，我们还规定：  神经体系结构(如果我们喜欢更改类提供的基本体系结构)。  输入Global_Step，这是一个全局变量，它将跟踪到目前为止提供给DQN网络的示例训练批次的数量。  用于存储TensorBoard日志的目录，TensorBoard是TensorFlow的标准可视化工具 

FileWriter命令将事件文件初始化为目标目录(CONLY_DIR)，我们将在该目录中存储学习过程的关键度量。句柄保持在自身中，稍后我们将使用它来存储我们有兴趣在培训期间和培训后所表示的措施，以监视和调试已经学到的内容。  下一个要定义的方法是我们将用于这个项目的默认神经网络。作为输入，它接受输入层和我们将要使用的隐藏层的各自大小。输入层是由我们正在使用的状态定义的，它可以是测量的向量，比如我们的例子，也可以是原始DQN文件中的图像)。  这些层是简单地使用更高级别的操作来定义的，这些操作是由。  TensorFlow(https：/www.tensorflow.org/api_guides/python/contrib.layer s).。我们选择完全连接的香草，使用两个隐藏层的RERU(整流器)激活函数和输出层的线性激活。  32的预定义大小非常适合我们的目的，但是如果您愿意，可以增加它。此外，这个网络也没有辍学现象。显然，这里的问题不是过于恰当，而是正在学习的内容的质量，只有通过提供有用的不相关状态序列和对最终相关回报的良好估计，才能提高学习质量。正是在有用的状态序列中，特别是在勘探和开发之间权衡的情况下，不使网络过度匹配的关键在于。在强化学习问题中，如果您陷入以下两种情况之一，您就已经过度适应了：  次优性：算法提出次优解，就是这样，我们的着陆器学会了一种粗略的着陆方法，它坚持下来，因为至少它着陆了。  无助：算法陷入了一种习得的无助状态，也就是说，它没有找到正确着陆的方法，所以它只是接受了它将以尽可能不坏的方式崩溃。  这两种情况对于强化学习算法(如DQN)来说确实很难克服，除非该算法有机会在游戏中探索其他解决方案。不时地采取随机行动并不是一种简单的把事情搞砸的方法，就像你第一眼看到的那样，而是一种避免陷阱的策略。  另一方面，对于比这个更大的网络，你可能会对一个垂死的神经元产生问题，需要你使用不同的神经元。  激活tf.nn.leepy_relu(https：/www.tensorflow.org/api_docs/python/tf/nn/leloy_relu)，以便获得一个工作网络。  死中继总是输出相同的值，通常是零值，并且它会对反向传播更新产生抗性。  从TensorFlow 1.4开始，就有了激活LECYY_RELLU。如果您正在使用以前版本的TensorFlow，您可以创建一个将在您的自定义网络中使用的特殊函数：  def leaky_relu(x，alpha=0.2)：返回tf.nn.relu(X)-alpha*tf.nn.relu(-x)。  我们现在开始对我们的大脑类进行编码，为它添加一些更多的功能： 

 方法CREATE_ANN结合了输入、神经网络、损失和优化。损失只是通过将原始奖励与估计结果之间的差取平方，并通过所学习的批次中的所有示例取平均值而造成的。使用ADAM优化器将损失最小化。  此外，还为TensorBoard记录了一些摘要：  批次的平均损失，以便在训练期间跟踪健康状况。  在批次中最大的预测奖励，以跟踪极端积极的预测，指出最成功的行动。  为了跟踪预测好动作的一般趋势，在批中平均预测奖励。  下面是Create_Network的代码，它是我们项目的TensorFlow引擎： 

 这个类是通过预测和拟合的方法完成的。该方法以状态矩阵s作为输入批次，以奖励r的向量作为结果。它还考虑到你想训练多少个历元(在最初的论文中，建议每批只使用一个历元，以避免对每一批观测值过高)。然后，在本届会议中，输入与结果和摘要(以前定义为我们创建网络)相适应。 

因此，将返回全局Step，这是一个计数器，可以帮助跟踪到目前为止在培训中使用的示例数量，然后记录下来供以后使用。  为经验重播创建内存。  在定义了大脑(TensorFlow神经网络)之后，我们下一步是定义内存，即数据存储，这将为DQN网络的学习过程提供动力。在每一集训练中，每一步，由一个状态和一个动作组成，与随后的状态和该集的最终奖励一起被记录下来(只有在该集结束时才会知道)。  添加一个标志，告诉观察是否是终端，这就完成了记录信息的集合。其想法是将某些动作不仅与即时奖励(可能为空或适度)，而且与结束奖励相关联，从而将该会话中的每一步与其关联起来。  类内存只是一个具有一定大小的队列，然后该队列中充满了关于以前游戏体验的信息，并且很容易对其进行采样和提取。考虑到它的固定大小，重要的是将较老的示例从队列中推出来，从而使可用的示例始终位于最后一个示例中。  这个类包括一个初始化，其中数据结构是原始的并且它的大小是固定的，len方法(因此我们知道内存是否满了，这是很有用的，例如，为了等待任何训练，至少要等到我们有足够的训练，以便更好地随机性和学习的多样性)，添加在队列中记录的内存，以及用列表格式从它恢复所有数据的RECARE_内存： 

创建代理。  下一个类是代理，它具有初始化和维护大脑(提供Q值函数近似)和内存的作用。此外，是代理人在环境中行动。它的初始化设置了一系列的参数，这些参数大多是固定的，因为我们在优化月球登陆者游戏的学习经验。但是，在第一次初始化代理时，可以显式更改它们：  Epsilon=1.0是勘探开发参数的初始值。1.0值迫使代理完全依赖于探索，即随机移动。  Epsilon_min=0.01设置勘探-开发参数的最小值：值0.01表示着陆舱随机移动的可能性为1%，而不是基于Q函数反馈。这总是提供了一个最小的机会找到另一个最优的方式完成游戏，而不损害它。  epsilon_衰变=0.9994是调节epsilon减小到最小速度的衰减。在这个设置中，在大约5，000集之后，它被调整到一个最小值，平均来说，这应该为算法提供至少200万个可供学习的示例。  伽玛=0.99是奖励折现因子，Q值估计值根据当前奖励来衡量未来奖励的权重，从而允许算法根据所玩游戏的最佳方式是短视或远视(在月球着陆器中，最好是远视，因为实际的奖励只在登月吊舱着陆时才会得到)。(。)。  LearingRate=0.0001是ADAM优化器学习这批示例的学习速率。  EPOCH=1是神经网络为适应批处理样本而使用的训练周期。BUT_SEAM=32是批示例的大小。REMERY=内存(REMERY_SIZE=250000)是内存队列的大小。“内存”=“内存”。  使用预置参数，可以确保当前项目将正常工作。对于不同的OpenAI环境，您可能需要找到不同的最佳参数。  初始化还将提供定义TensorBoard日志放置位置(默认情况下是实验目录)所需的命令、学习如何估计立即下一个奖励的模型，以及存储最终奖励的权重的另一个模型。此外，将初始化一个保存程序(tf.tra.saver)，允许将整个会话序列化到磁盘，以便以后恢复它并将其用于玩真正的游戏，而不仅仅是学习如何玩它。  上述两个模型在同一个会话中初始化，使用不同的作用域名称(一个是Q，由TensorBoard监视的下一个奖励模型；另一个是Target_Q)。使用两个不同的作用域名称可以方便地处理神经元的系数，从而可以用类中的另一个方法交换它们： 

与利用网络知识相比，用于探索新解决方案的时间份额的epsilon处理不断更新epsilon_update方法，该方法只是通过将实际epsilon乘以epsilon_衰变来修改实际epsilon，除非它已经达到了允许的最小值： 

SET_LOGER和TARGET_MODEL_UPDATE方法一起工作，用Q网络的权重来更新目标Q网络(SET_WALL是一个通用的、可重用的函数，您也可以在解决方案中使用)。由于这两个作用域的命名不同，因此很容易从可训练变量列表中枚举每个网络的变量。一旦枚举，这些变量就会加入到要由正在运行的会话执行的赋值中： 

ACT方法是策略执行的核心，因为它将基于epsilon来决定是采取随机移动还是进行尽可能好的移动。如果它正朝着尽可能好的方向移动，它将要求经过训练的Q网络为每个可能的下一步动作提供一个奖励估计(通过在月球着陆器游戏中按四个按钮中的一个以二进制方式表示)，它将返回以最大预测奖励(解决方案的贪婪方法)为特征的移动： 

重播方法完成类。这是一个关键的方法，因为它使学习的DQN算法成为可能。因此，我们将深入讨论它是如何运作的。重播方法所做的第一件事是从先前游戏场景的内存中取样一批(我们在初始化时定义了批的大小)(这些内存只是包含状态、动作、奖励、下一个状态的值的变量，以及注意观察是否是最终状态的标志变量)。随机抽样使模型能够通过对网络权值的缓慢调整，一批又一批地找出最优的系数，从而学习Q函数。  然后，该方法确定抽样召回状态是否是最终的。非最终奖励需要更新，以表示您在游戏结束时得到的奖励。这是通过使用目标网络来完成的，目标网络表示上一次学习结束时固定的Q函数网络的快照。向目标网络提供以下状态，在用伽玛因子折现后，将得到的报酬与当前报酬相加。  使用当前的Q函数可能会导致学习过程的不稳定性，而不会导致一个令人满意的Q函数网络。 

当非终端状态的奖励被更新时，批数据被输入到神经网络中进行训练。  指定环境。  要实现的最后一个类是环境类。实际上，环境是由健身房命令提供的，尽管您需要一个好的包装器来使它与前面的代理类一起工作。这正是这堂课要做的。在初始化时，它启动月球着陆器游戏，并设置关键变量，如NS、NA(状态和动作的维度)、代理和累积奖励(通过提供最后100集的平均值来测试解决方案)： 

 然后，我们编写了测试、训练和增量(增量训练)方法的代码，这些方法被定义为综合学习方法的包装器。  使用增量训练是有点棘手，它需要一些注意，如果你不想破坏你已经取得的结果，从你的训练到目前为止。问题是，当我们重新启动时，大脑有预先训练过的系数，但实际上记忆是空的(我们可以称之为冷重启)。由于代理的内存是空的，它不能支持良好的学习，因为太少和有限的例子。因此，所提供的示例的质量对于学习来说确实不是完美的(这些示例大部分是相互关联的，并且非常适合于少数几个新体验的场景)。通过使用非常低的epsilon(我们建议将其设置为最低，0.01)，可以减少破坏培训的风险：通过这种方式，网络将在大多数情况下简单地重新学习自己的权重，因为它将为每个状态建议它已经知道的操作，并且它的性能不应该恶化，而是在内存中有足够的示例之前以稳定的方式振荡，并且它将再次开始改进。  下面是发布正确的培训和测试方法的代码： 

最后一种方法是学习，将所有步骤安排好，让代理与环境交互并从中学习。该方法取epsilon值(从而覆盖代理拥有的任何先前的epsilon值)、在环境中运行的事件数、是否对其进行了培训(布尔标志)，以及培训是否从以前的模型(另一个布尔标志)的培训中继续进行。  在第一个代码块中，如果我们想要的话，该方法为Q值近似加载先前训练的网络权重：  1.测试网络，看看它是如何工作的； .利用进一步的例子进行一些以前的训练。 然后，该方法深入到嵌套迭代中。外部迭代正在运行所需的剧集数量(每一集月球着陆器游戏已采取其结论)。而内部迭代则是经过最多1000个步骤组成的一个插曲。 在迭代的每一步中，神经网络在下一步中被询问。如果它正在测试中，它总是简单地提供下一个最佳动作的答案。如果它正在训练中，根据epsilon的值，它可能不会建议最好的动作，但它会建议随机移动。 

在移动之后，所有信息被收集(初始状态、选择的动作、获得的奖励和随后的状态)并保存到内存中。在这个时间框架中，如果内存足够大，可以为逼近Q函数的神经网络创建批处理，则运行一个培训课程。当该插曲的所有时间帧都被消耗完时，DQN的权重被存储到另一个网络中，当DQN网络从一个新插曲中学习时，作为一个稳定的参考。 运行强化学习过程。 最后，在所有关于强化学习和DQN的离题之后，并编写了项目的完整代码之后，您可以使用脚本或朱庇特笔记本来运行它，利用将所有代码功能放在一起的环境类：登月_着陆器=环境(游戏=“月球着陆器-v2”)。 在实例化之后，您只需运行列车，从epsilon=1.0开始，并将目标设置为5000集(这相当于大约220万个状态、行为和奖励的链式变量示例)。我们提供的实际代码被设置为成功地完成一个经过充分训练的DQN模型，尽管考虑到GPU的可用性及其计算能力，它可能需要一些时间：登月舱(epsilon=1.0，剧集=5000)。 最后，该类将完成所需的培训，将保存的模型留在磁盘上(可以随时运行，甚至可以进行报复)。您甚至可以使用一个简单的命令来检查TensorBoard，该命令可以从shell运行：tensorboard-logdir=./test-端口6006。 这些地块将出现在您的浏览器上，并且可以在本地地址localhost：6006上查看它们： 

图4：沿着训练的损失趋势，峰值代表学习中的突破思想，例如在800 K的例子时，它开始安全地降落在地面上。  损失图显示，与其他项目不同的是，优化的特点仍然是损失减少，但在这一过程中出现了许多峰值和问题：  这里表示的图表是运行该项目一次的结果。由于该过程中有一个随机组件，因此在您自己的计算机上运行项目时，您可能会获得稍微不同的绘图。  图5：在批量学习会话中获得的最大Q值的趋势。  最大预测Q值和平均预测Q值说明了同样的情况。该网络在最后得到了改进，尽管它可以稍微回溯它的步骤，并在高原上停留很长一段时间：  只有在最后100个最终奖励的平均值中，您才能看到一条增量路径，这暗示了DQN网络的持续和稳步改进：  图7：每个学习阶段结束时实际获得分数的趋势，它更清楚地描述了DQN不断增长的能力。  使用来自输出的相同信息，而不是来自TensorBoard的信息，您还会发现操作的数量根据epsilon值的平均值而变化。开始时，完成一集所需的动作次数少于200次。突然，当epsilon为0.5时，平均动作数趋于稳定增长，在750左右达到峰值(着陆舱已经学会用火箭来抵消重力)。  最后，网络发现这是一个次优策略，当epsilon低于0.3时，完成一集的平均操作数也会下降。在这一阶段，DQN正在探索如何以更有效的方式成功地着陆吊舱：  图8：epsilon(勘探/开发率)与DQN网络效率之间的关系，表示为完成一集所使用的一些步骤。  如果出于任何原因，您认为网络需要更多的示例和学习，您可以使用增量方法重复学习，记住在这种情况下epsilon应该非常低：  月球着陆器.增量(插曲=25，epsilon=0.01)。  训练结束后，如果您需要查看结果并了解平均每100集DQN可以得分多少(理想的目标是分数>=200)，您可以运行以下命令：  月球着陆器.test()。  致谢。  在这个项目结束时，我们确实要感谢Peter Skvarenina，他的项目“月球登陆者II”(https：/www.youtu.com/Watch？v=yiAmrzuBaY U)是我们自己的项目的主要灵感来源，也是他在制作我们自己的深Q-网络版本过程中提出的所有建议和提示。  摘要。  在这个项目中，我们探索了增强算法在OpenAI环境中可以实现什么，并且我们编写了一个TensorFlow图，它能够学习如何在一个以代理、状态、动作和随后的奖励为特征的环境中估计最终的奖励。这种方法称为DQN，目的是用神经网络方法逼近Bellman方程的结果。结果是一个月球着陆器游戏，该软件可以在训练结束时通过读取游戏状态并决定在任何时候采取正确的行动来成功地玩。 